items:

- id: ricardo-ferreira
  name: Ricardo Ferreira
  bio: |
    Ricardo is Observability Lead for Elastic’s community team, where he acts as the voice of Elastic at SRE, DevOps, and DataOps communities. In this role, he is responsible for developing and driving a global community and developer advocacy strategy focused on observability. With +20 years of experience, he might have learned a thing or two about distributed systems, observability, streaming systems, and databases. Before Elastic, he worked for other vendors such as Confluent, Oracle, Red Hat, and different consulting firms.

    While not working, he loves barbecuing in his backyard with his family and friends, where he gets the chance to talk about anything that is not IT-related. He lives in North Carolina, USA, with his wife and son.
  role: presenter
  title: Building Software Reliability with Distributed Tracing
  abstract: |
    Most developers believe that building reliable software involves writing good code, implementing enough testing, and using as many proven architecture patterns as possible. The assumption is that building things this way equals creating a flawless system. Sadly, in the software world, this is not true. Software reliability is not the same as software correctness. You may write good code, implement enough testing, and use as many proven architecture patterns as possible to end up with software deemed correct. But, the code may still blow up straight on the customer’s face.

    Building software reliability is something else entirely. It requires developers to look at the code, not from the perspective of what it is supposed to do but what the code is effectively doing, with little room for guessing. Distributed tracing is a technique that developers can use to accomplish this.

    This talk will discuss how distributed tracing can positively change how you and your team deliver customers’ software. After all, five-nines of availability don’t matter if users aren’t happy. It will highlight which techniques with distributed tracing bring value, what to focus on and what avoid, and the most common challenges. By the end, you will know everything you need to adopt distributed tracing as a practice that promotes software reliability.
  handles:
    twitter: riferrei
    linkedin: riferrei
  session_url: https://vi.to/hubs/o11yfest/videos/4896
  preactions:
  - contributor: Leticia Mendonca
    youtube_id: z58Sv0QaHvo
    video_url: https://www.youtube.com/watch?v=z58Sv0QaHvo

- id: michael-haberman
  name: Michael Haberman
  bio: |
    Michael is the Co-Founder and CTO of Aspecto. After serving as a software developer in an elite unit in the Israeli Intelligence branch, Michael worked with a few startups on building and scaling their microservices infrastructure. Prior to co-founding Aspecto, he was the Chief Architect at Playbuzz. In his free time, Michael also lectures and conducts workshops on microservices at conferences.
  role: presenter
  title: Trace-Based Testing with OpenTelemetry
  abstract: |
    Distributed tracing allows devs to find and fix issues in production after they happen. There is another use case for tracing data: trace-based testing. We can improve assertion capabilities by leveraging OpenTelemetry trace data and making it accessible while setting our expectations from a test.

    Companies these days use distributed tracing for critical functions such as performance monitoring and troubleshooting, allowing DevOps, developers, and SREs to find and fix issues in production after they happen. But here is the thing, they don’t use tracing to its full potential.

    There is another use case for tracing data, and that is trace-based testing. This new method allows you to improve assertion capabilities by leveraging traces data and making it accessible while setting your expectations from a test.

    We will introduce you to a new open-source called Malabi - a Javascript framework based on OpenTelemetry that allows you to easily use trace data to take your assertion capabilities to the next level.

    By the end of this session, you will know how to use trace-based testing to increase your tests’ reliability and possibly prevent issues early in the development cycle.
  handles:
    twitter: hab_mic
    linkedin: michael-haberman
  session_url: https://vi.to/hubs/o11yfest/videos/4895
  preactions:
  - contributor: Lewis Prescott
    youtube_id: hL9sHmBPleU
    video_url: https://www.youtube.com/watch?v=hL9sHmBPleU

- id: jessica-kerr
  name: Jessica (Jessitron) Kerr
  bio: |
    Jessica Kerr is a developer advocate, software developer and symmathecist with 20+ years of experience. She has worked in enterprises and startups, in Java, Scala, Clojure, Ruby, and TypeScript. Talk to her about technical details, or about how to get software to teach us about its needs.
  role: presenter
  title: Observability During Development
  abstract: |
    How will our development flow be different once we take observability for granted? Jess demonstrates what coding is like when observability is a natural part of the flow. Get insight into what the code is doing immediately, and keep that insight in production. Is 'create span' the new 'printf'?
  handles:
    twitter: jessitron
    linkedin: jessicakerr

- id: colin-douch
  name: Colin Douch
  bio: |
    Colin Douch is an SRE at Cloudflare, tech leading their Observability Platform, with over 10 years experience working across DevOps and SRE organisations. Originally from New Zealand, but currently living in Australia (and trying to blend in), he has worked with organisations both big and small to improve their Observability systems.
  role: presenter
  title: High Cardinality Alerting With Open Telemetry
  abstract: |
    A common problem with our existing alerting systems is that they are limited by the cardinality issues inherent in Time Series Databases. This allows them to provide a very quick signal of when something is wrong, but causes them to fail to provide enough context as to exactly what that is. The result is that the first steps of debugging an alert are generally blindly searching through higher cardinality data sources such structured logs and traces to further debug the issue. But what if it didn’t have to be that way? At Cloudflare, as part of our transition towards a tracing first Observability system, we have developed a system - “Cleodora” - that allows aggregating time series data, in memory, or persistently in Clickhouse from OpenTelemetry traces. Cleodora then allows us to create alerting rules over these aggregates, directly into our existing Alertmanager setup. This allows us to utilise the full context of each traces high dimensionality and high cardinality labels for alerting purposes, providing deeper context on alerts and allowing our engineering teams to more quickly identify and fix the root cause of incidents.

    In this talk, Colin will explain what led to Cleodoras development, where it fits into Cloudflares monitoring stack, and the benefits that it has provided; reducing the load on our Prometheus servers and providing a stepping stone to tracing introduction, allowing us to further our distributed tracing offerings. He will further discuss where Cleodora is going from here, and how other organisations can use it to start their own transitions towards a proper Observability system.
  handles:
    twitter: sinkingpoint
    linkedin: colin-douch-82940b120

- id: michael-hausenblas
  name: Michael Hausenblas
  bio: |
    Michael is a Solution Engineering Lead in the AWS open source observability service team. He covers Prometheus, Grafana, and OpenTelemetry upstream and in managed services. Before Amazon, Michael worked at Red Hat, Mesosphere (now D2iQ), MapR (now part of HPE), and prior to that ten years in applied research.
  role: presenter
  title: Return on Investment driven observability
  abstract: |
    No one wants to fly blind when developing & operating cloud native apps. But how do you select the signals (logs, metrics, traces, profiles) to use for a task and how can you assess how much bang you get for the buck (instrumentation, agents, destinations)? In this talk we aim to provide answers.

    In the context of developing and operating cloud native apps, such as Kubernetes workloads or AWS Lambda-based app, no one wants to fly blind. But how do you select the signals (logs, metrics, traces, profiles) to use for a certain task such as troubleshooting or performance optimization? Also, how can you assess how much bang you get for the buck (in terms of costs of instrumentation, for agents, and destinations including long-term storage)? In this talk we aim to provide answers to these questions, suggesting a “Return on Investment”- driven observability approach.
  handles:
    twitter: mhausenblas
    linkedin: mhausenblas

- id: henrik-rexed
  name: Henrik Rexed
  bio: |
    Henrik is a Cloud Native Advocate at Dynatrace, the leading Observability platform. Prior to Dynatrace, Henrik has  worked as a Partner Solution Evangelist at Neotys, delivering webinars, building protypes to enhance the capability of NeoLoad.  He has been working in the performance world more than 15 years, delivering projects in all contexts including extremely large Cloud testing on the most demanding business areas such as trading applications, Video on Demand, sports websites, etc.

    Henrik Rexed is also one of the Organizer of the Conference named Performance Advisory Council , one of the producers of PerfBytes and the owner of the Youtube Channel IsitObservable.
  role: presenter
  title: "The Sound of Code: Instrument with OpenTelemetry"
  abstract: |
    To be efficient in our Continuous testing, continuous deployment approach and on the way we handle production outage, we need to have the right level of observability to understand properly how:

    Our users are interacting with our application
    Our system behaves with significant traffic
    Our infrastructure has been utilize by our application
    …etc
    To help us in our journey, OpenTelemetry has provided a standard on the way we are able to produce and collect measurements in our environments.

    Like any other technology transformation, OpenTelemetry adoption typically starts with small “pet projects” where we usually try to utilize manual or automatic instrumentation. But Opentelemetry is not only about producing traces, it covers the ability to generate metrics and logs from our applications. But like any new emerging framework we tend to wait to get a stable implementation before utilizing it in our production environment.

    We all understand the value of having traces generated out of our application, but the journey could seem difficult and time consuming.

    How can we utilize properly the instruments of Opentelemtry to make our code sound beautiful?

    During this presentation we will explain:
    - The various components of OpenTelemetry
    - The various core Objects required to instrument your code
    - Best practices related to instrumentation
    - The latest news related to OpenTelemetry

  handles:
    twitter: hrexed
    linkedin: hrexed

- id: shai-almog
  name: Shai Almog
  bio: |
    Developer advocate for [Lightrun](https://www.lightrun.com), co-founder of [Codename One](https://www.codenameone.com/), Creator of [DDT](https://github.com/ddtj/ddtj), open source hacker, speaker, author, blogger, Java rockstar and more. ex-Sun/Oracle guy with 30 years of professional development experience. Shai built virtual machines, development tools, mobile phone environments, banking systems, startup/enterprise backends, user interfaces, development frameworks and much more.
  role: presenter
  title: "Debugging at Scale in Production - Deep into your Containers with kubectl debug, KoolKits and Continuous Observability"
  abstract: |
    Brian Kernigham said: “Debugging is twice as hard as writing the code in the first place.”

    In fact, debugging in a modern production environment is even harder - orchestrators spinning containers up and down and weird networking wizardry that keeps everything glued together, make understanding systems that much more difficult than it used to be.

    And, while k8s is well understood by DevOps people by now, it remains a nut that developers are still trying to crack. Where do you start when there’s a production problem? How do you get the tools you’re used to in the remote container? How do you understand what is running where and what is its current state?

    In this talk, we will review debugging a production application deployed to a Kubernetes cluster, and review kubectl debug - a new feature from the Kubernetes sig-cli team. In addition, we’ll review the open source KoolKits project that offers a set of (opinionated) tools for kubectl debug.

    KoolKits builds on top of kubectl debug by adding everything you need right into the image. When logging into a container, we’re often hit with the scarcity of tools at our disposal. No vim (for better or worse), no DB clients, no htop, no debuggers, etc… KoolKits adds all the tools you need right out of the box and lets you inspect a production container easily without resorting to endless installation and configuration cycles for each needed package.

    We’ll finish the talk by delving into how to get better at debugging on a real-world scale. Specifically, we’ll talk about how to be disciplined in our continuous observability efforts by using tools that are built for k8s scale and can run well in those environments, while remaining ergonomic for day to day use.

    This session will go back and forth between explanation slides and demonstration of the topic at hand.
  handles:
    twitter: debugagent
    linkedin: shai-almog-81a42
  session_url: https://vi.to/hubs/o11yfest/videos/5009

# - id: adrian-gonzalez
#   name: Adrian Gonzalez
#   bio: |
#     [TBD]
#   role: presenter
#   title: "Seeing Your Product Succeed through User Telemetry"
#   abstract: |
#     The business value of observability for many enterprise customers comes in the form of user telemetry. Every business wants to know, 'Who are my users, when are they using, why, how, and what buttons are they touching?". This understanding can determine or define the success of your product.
#   handles:
#     linkedin: adrian-g-gonzalez

- id: zoe-steinkamp
  name: Zoe Steinkamp
  bio: |
    Zoe Steinkamp is a Developer Advocate for InfluxData. She has worked for InfluxData as a front end software engineer for over two years. Before InfluxData, she worked as a front end engineer for over 5 years in the original AngularJS. She originally went to a bootcamp for training in Python. Her favorite activities outside of work include traveling and gardening.
  role: presenter
  title: "Cleaning and Interpreting Time-series Metrics with InfluxDB"
  abstract: |
    Metric data can benefit from being standardized and condensed before being stored in your database. It can also be useful to be able to search and filter the data outside of your database. The flux data processing language is built for handling these tasks in the Flux VS code tool.

    Raw time series metrics data can benefit from clean-up and normalization before exposing it for broader use and storage. When dealing with large amounts of time series metrics, it can be helpful to be able to standardize the ways in which others can search through that data for specific time frames using easy to understand tags. This talk focuses on using Flux, InfluxDB’s data processing language, for addressing these challenges. Examples of how to leverage Flux to accomplish data cleansing and analytics through the browser and via Visual Studio will be demonstrated.
  handles:
    linkedin: zoe-steinkamp

- id: daniel-selans
  name: Daniel Selans
  bio: |
    Dan is a co-founder of Batch.sh, a data stream observability company. Dan previously worked at InVisionApp, New Relic and before that, spent some time doing integration work at data centers. He has been writing Go for 6+ years, works primarily in backend, listens to a lot of black metal and prefers Stella's over IPA's. He resides in Portland, Oregon but is originally from [Latvia](https://goo.gl/maps/9wruLSg4RBU2).
  role: presenter
  title: "Observability in Event Driven *"
  abstract: |
    Event driven systems are on the rise but there is still a significant lack of information about how to observe these complex systems.

    Event driven is not new - there are several well-known articles written about it but it remains difficult to find up-to date information on the various paradigms - architecture, implementation, component breakdowns or just some guidance on how to approach observability.

    In this talk, I will introduce the listener to event driven concepts and share observability patterns that have worked for our team.

    This information comes from first-hand experience designing, implementing and operating an event-driven system that our company uses to process over ten billion (protobuf) events per day.

    Learn how to set yourself up for success when working with async-heavy systems.

    (Rough) Outline of the talk:

    1. Introduction to event driven concepts

    2. What is event driven? (<5 minutes)

    3. Component breakdown (<5 minutes)

    4. Observing Async

    5. Why is it hard? (<=5 minutes)

    6. Patterns that work (10 minutes)

    7. What to avoid (5 minutes)

  handles:
    twitter: skey
    linkedin: dselans

- id: stephen-townshend
  name: Stephen Townshend
  bio: |
    Stephen pretended to be a performance engineer for thirteen years, and very recently started pretending to be an SRE. He is actually an actor, playing the role of a site reliability engineer. At some point along the way he lost touch with reality and is no longer sure if he is acting, or this has become reality. In the words of Robert Downey, Jr. in the film Tropic Thunder: “I know who I am. I’m a dude playing a dude disguised as another dude.”
  role: presenter
  title: "Bad Observability"
  abstract: |
    What are some antipatterns can hurt the success of our observability? What does “bad” observability look like?

    From my experience, most organizations have a lot of monitoring. And most of the time that monitoring isn’t able to answer basic questions about customer behaviour and experience, and isn’t being used as part of a feedback loop to pivot and make better business decisions.

    In this session I will explore both technical and cultural antipatterns that hinder observability. For example: - Lots of data but no insights - Monitoring a lot of technical metrics but not tracking customer behaviour and the impact of changes - Not knowing what the desired level of service is for the customer, or tracking it, or responding to it as part of a feedback loop - Misunderstanding what aggregates (averages, percentiles, etc.) mean and do not mean, or the impact of sampling intervals - Misunderstanding what certain metrics mean (for example, available memory on CPU’s, % CPU usage on containers and VM’s) - Siloed teams who do not share their monitoring with others …and many more.

    In this session my goal is to bring us back to outcomes. Rather than “do monitoring” for the sake of it, let’s be thoughtful about what we choose to measure and how we deal with the data that comes back, so that it drives better customer and business outcomes.

  handles:
    twitter: the_kiwi_sre
    linkedin: stephentownshend
  session_url: https://vi.to/hubs/o11yfest/videos/5008

- id: andreas-grabner
  name: Andreas Grabner
  bio: |
    Andreas Grabner has 20+ years of experience as a software developer, tester and architect and is an advocate for high-performing cloud scale applications. He is a contributor and DevRel for the CNCF open source project keptn (www.keptn.sh). Andreas is also a regular contributor to the DevOps community, a frequent speaker at technology conferences and regularly publishes articles on blog.dynatrace.com or medium. In his spare time you can most likely find him on one of the salsa dancefloors of the world (will resume once Covid is behind us)!
  role: presenter
  title: "Keptn: Putting Observability in the driving seat for DevOps & SRE automation"
  abstract: |
    Observability is the foundation giving us insights on how our systems currently behave. Keptn - a CNCF project - uses o11y to automate decisions in your delivery and ops automation.

    Its simple: You define SLOs! Keptn takes care of orchestrating your tools to deploy, test or release based on o11y!

    DevOps Platform Engineers and SREs are spending a lot of time integrating observability into their delivery and operations automation to automate decisions. This is done through lots of custom scripting, e.g: pull data from Jenkins, GitLab or Argo pipelines to make automated decisions based on the data coming out of Prometheus, OpenTelemetry ...

    Keptn standardizes how to pull data from your observability platform, how to analyze it by applying the concept of SLOs (Service Level Objectives) and how to integrate with your delivery, test, notification and configuration management tools.

    Keptn adopters have seen up to 90% reduction in custom coding to make observability a core component of their automation.

    In this session you learn how Keptn works, how the community is adopting it, how to integrate your own observability framework, how you can get started in minutes and how you can contribute to this open-source project.

  handles:
    twitter: grabnerandi
    linkedin: grabnerandi
  session_url: https://vi.to/hubs/o11yfest/videos/5007

- id: martin-mao
  name: Martin Mao
  bio: |
    Martin Mao is the co-founder and CEO of Chronosphere. He was previously at Uber, where he led the development and SRE teams that created and operated M3. Prior to that, he was a technical lead on the EC2 team at AWS and has also worked for Microsoft and Google. He and his family are based in our Seattle hub and he enjoys playing soccer and eating meat pies in his spare time.
  role: presenter
  title: "Is MTTR still relevant in a modern, cloud native world?"
  abstract: |
    MTTR has long been an essential failure metric. However, in a cloud native world, P95 and P99 have become more meaningful measurements. And time to remediation -not repair- is most important. During the talk, Martin will share an alternative to MTTR and how it can become your new P99 of remediation.

    Mean time to repair (MTTR) has long been an essential failure metric measuring the average time it takes to repair or restore a system to functionality. But why, in the age of microservices and containers, are we still using a metric with its origins in measuring equipment failures within factories? Mean, or average, is no longer a relevant metric for most organizations, with P95 and P99 becoming the more meaningful measurement. Repair, or sometimes restore, is also problematic. In most cases the most important time period to measure is the time to remediation, or the time to alleviate customer pain, restoring the service to acceptable levels of availability and performance. In this session, Martin will introduce an alternative to MTTR, and share real-life examples and lessons learned to explain how this new way of thinking can become your new P99 of remediation time.

  handles:
    twitter: martin_c_mao
    linkedin: martinmao

- id: piyush-verma
  name: Piyush Verma
  bio: |
    Piyush Verma is co-founder and CTO at Last9.io, an SRE platform that aims to minimize the toil that SREs and decision-makers need to go through to reduce the time to make a decision. Earlier, he led SRE @ TrustingSocial.com to produce 600 million credit scores a day across 4 countries. In his past life, he built oogway.in (exit to TrustingSocial.com), datascale.io (exit to Datastax), and siminars.com.
  role: presenter
  title: "Building OpenMetrics Exporter"
  abstract: |
    Openmetrics-exporter (OME) is an Observability-as-Code software that metrics what Terraform is to Cloud Operations. OME uses HCL configs written in HCL syntax to connect to cloud-native sources and collate metrics using simple pipes, absorbing all challenges of distributed systems underneath.

    Openmetrics-exporter, or OME, is an Observability-as-Code framework that reduces the toil of finding-and-combining useful metrics from layers and hundreds of components involved in modern cloud-native systems. Every source, component, or metric is just a simple configuration file because the only ‚Äúcode‚Äù you should focus on is for your customers.

    It leverages plugin architecture to support data sources. It relies heavily on data frame processing to combine metrics from various metrics sources before they are all converted into Openmetrics format, ready to be piped out by a Prometheus. Traditionally, such correlation and post-processing have been a responsibility of additional Data Pipelines but with OME it's as simple as writing a configuration file. At the core of it, OME uses Hashicorp Configuration Language (HCL) to build a DSL that can allow declarative input to build metric Pipelines.

    The talk is largely about what can you solve using OME. But it also takes a very short journey of ""behind-the-scenes""

    The need to build Openmetrics-exporter, picking a configuration language that was easily editable by humans, building a DSL around it, and more importantly leveraging Golang for Data Science needs.
  handles:
    twitter: realmeson10
    linkedin: meson10

- id: alex-boten
  name: Alex Boten
  bio: |
    Alex Boten is a Senior Staff Software Engineer at Lightstep and has spent the last ten years helping organizations adapt to a cloud-native landscape. From building core network infrastructure to mobile client applications and everything in between, Alex has first-hand knowledge of how complex troubleshooting distributed applications is.

    This led him to the domain of observability and contributing to open-source projects in the space. A contributor, approver, and maintainer in several aspects of OpenTelemetry, Alex has helped evolve the project from its early days in 2019 into the massive community effort that it is today.

    More than anything, Alex loves making sense of the technology around us and sharing his learnings with others.
  role: presenter
  title: "How the OpenTelemetry Collector puts you in the driver seat"
  abstract: |
    After spending hours, days, weeks of effort ensuring applications are producing good telemetry, the last thing anyone wants is to touch that code ever again. As needs change, you want the flexibility to try new solutions. What if there was an easy way to do this? Enters the OpenTelemetry Collector.

    Instrumenting code is a pain. OpenTelemetry provides the tooling necessary to instrument code once and for all. Gone are the days of vendor lock-in, everyone can now use an open standard. The OpenTelemetry Collector provides even more control over your data, allowing you to send data to any number of telemetry backends without ever touching application code.

    In this talk, you‚'ll learn:

    * What the OpenTelemetry Collector is

    * How it provides maximum control & flexibility for your data

    * The ways you can extend the Collector

    It's your data! Retain control of it with the Collector.
  handles:
    twitter: codeboten
    linkedin: codeboten
