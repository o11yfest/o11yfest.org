items:

- id: ricardo-ferreira
  name: Ricardo Ferreira
  bio: |
    Ricardo is Observability Lead for Elastic’s community team, where he acts as the voice of Elastic at SRE, DevOps, and DataOps communities. In this role, he is responsible for developing and driving a global community and developer advocacy strategy focused on observability. With +20 years of experience, he might have learned a thing or two about distributed systems, observability, streaming systems, and databases. Before Elastic, he worked for other vendors such as Confluent, Oracle, Red Hat, and different consulting firms.

    While not working, he loves barbecuing in his backyard with his family and friends, where he gets the chance to talk about anything that is not IT-related. He lives in North Carolina, USA, with his wife and son.
  role: presenter
  title: Building Software Reliability with Distributed Tracing
  abstract: |
    Most developers believe that building reliable software involves writing good code, implementing enough testing, and using as many proven architecture patterns as possible. The assumption is that building things this way equals creating a flawless system. Sadly, in the software world, this is not true. Software reliability is not the same as software correctness. You may write good code, implement enough testing, and use as many proven architecture patterns as possible to end up with software deemed correct. But, the code may still blow up straight on the customer’s face.

    Building software reliability is something else entirely. It requires developers to look at the code, not from the perspective of what it is supposed to do but what the code is effectively doing, with little room for guessing. Distributed tracing is a technique that developers can use to accomplish this.

    This talk will discuss how distributed tracing can positively change how you and your team deliver customers’ software. After all, five-nines of availability don’t matter if users aren’t happy. It will highlight which techniques with distributed tracing bring value, what to focus on and what avoid, and the most common challenges. By the end, you will know everything you need to adopt distributed tracing as a practice that promotes software reliability.
  handles:
    twitter: riferrei
    linkedin: riferrei

- id: michael-haberman
  name: Michael Haberman
  bio: |
    Michael is the Co-Founder and CTO of Aspecto. After serving as a software developer in an elite unit in the Israeli Intelligence branch, Michael worked with a few startups on building and scaling their microservices infrastructure. Prior to co-founding Aspecto, he was the Chief Architect at Playbuzz. In his free time, Michael also lectures and conducts workshops on microservices at conferences.
  role: presenter
  title: Trace-Based Testing with OpenTelemetry
  abstract: |
    Distributed tracing allows devs to find and fix issues in production after they happen. There is another use case for tracing data: trace-based testing. We can improve assertion capabilities by leveraging OpenTelemetry trace data and making it accessible while setting our expectations from a test.

    Companies these days use distributed tracing for critical functions such as performance monitoring and troubleshooting, allowing DevOps, developers, and SREs to find and fix issues in production after they happen. But here is the thing, they don’t use tracing to its full potential.

    There is another use case for tracing data, and that is trace-based testing. This new method allows you to improve assertion capabilities by leveraging traces data and making it accessible while setting your expectations from a test.

    We will introduce you to a new open-source called Malabi - a Javascript framework based on OpenTelemetry that allows you to easily use trace data to take your assertion capabilities to the next level.

    By the end of this session, you will know how to use trace-based testing to increase your tests’ reliability and possibly prevent issues early in the development cycle.
  handles:
    twitter: hab_mic
    linkedin: michael-haberman

- id: jessica-kerr
  name: Jessica (Jessitron) Kerr
  bio: |
    Jessica Kerr is a developer advocate, software developer and symmathecist with 20+ years of experience. She has worked in enterprises and startups, in Java, Scala, Clojure, Ruby, and TypeScript. Talk to her about technical details, or about how to get software to teach us about its needs.
  role: presenter
  title: Observability During Development
  abstract: |
    How will our development flow be different once we take observability for granted? Jess demonstrates what coding is like when observability is a natural part of the flow. Get insight into what the code is doing immediately, and keep that insight in production. Is 'create span' the new 'printf'?
  handles:
    twitter: jessitron
    linkedin: jessicakerr

- id: colin-douch
  name: Colin Douch
  bio: |
    Colin Douch is an SRE at Cloudflare, tech leading their Observability Platform, with over 10 years experience working across DevOps and SRE organisations. Originally from New Zealand, but currently living in Australia (and trying to blend in), he has worked with organisations both big and small to improve their Observability systems.
  role: presenter
  title: High Cardinality Alerting With Open Telemetry
  abstract: |
    A common problem with our existing alerting systems is that they are limited by the cardinality issues inherent in Time Series Databases. This allows them to provide a very quick signal of when something is wrong, but causes them to fail to provide enough context as to exactly what that is. The result is that the first steps of debugging an alert are generally blindly searching through higher cardinality data sources such structured logs and traces to further debug the issue. But what if it didn’t have to be that way? At Cloudflare, as part of our transition towards a tracing first Observability system, we have developed a system - “Cleodora” - that allows aggregating time series data, in memory, or persistently in Clickhouse from OpenTelemetry traces. Cleodora then allows us to create alerting rules over these aggregates, directly into our existing Alertmanager setup. This allows us to utilise the full context of each traces high dimensionality and high cardinality labels for alerting purposes, providing deeper context on alerts and allowing our engineering teams to more quickly identify and fix the root cause of incidents.

    In this talk, Colin will explain what led to Cleodoras development, where it fits into Cloudflares monitoring stack, and the benefits that it has provided; reducing the load on our Prometheus servers and providing a stepping stone to tracing introduction, allowing us to further our distributed tracing offerings. He will further discuss where Cleodora is going from here, and how other organisations can use it to start their own transitions towards a proper Observability system.
  handles:
    twitter: sinkingpoint
    linkedin: colin-douch-82940b120

- id: michael-hausenblas
  name: Michael Hausenblas
  bio: |
    Michael is a Solution Engineering Lead in the AWS open source observability service team. He covers Prometheus, Grafana, and OpenTelemetry upstream and in managed services. Before Amazon, Michael worked at Red Hat, Mesosphere (now D2iQ), MapR (now part of HPE), and prior to that ten years in applied research.
  role: presenter
  title: Return on Investment driven observability
  abstract: |
    No one wants to fly blind when developing & operating cloud native apps. But how do you select the signals (logs, metrics, traces, profiles) to use for a task and how can you assess how much bang you get for the buck (instrumentation, agents, destinations)? In this talk we aim to provide answers.

    In the context of developing and operating cloud native apps, such as Kubernetes workloads or AWS Lambda-based app, no one wants to fly blind. But how do you select the signals (logs, metrics, traces, profiles) to use for a certain task such as troubleshooting or performance optimization? Also, how can you assess how much bang you get for the buck (in terms of costs of instrumentation, for agents, and destinations including long-term storage)? In this talk we aim to provide answers to these questions, suggesting a “Return on Investment”- driven observability approach.
  handles:
    twitter: mhausenblas
    linkedin: mhausenblas

- id: henrik-rexed
  name: Henrik Rexed
  bio: |
    Henrik is a Cloud Native Advocate at Dynatrace, the leading Observability platform. Prior to Dynatrace, Henrik has  worked as a Partner Solution Evangelist at Neotys, delivering webinars, building protypes to enhance the capability of NeoLoad.  He has been working in the performance world more than 15 years, delivering projects in all contexts including extremely large Cloud testing on the most demanding business areas such as trading applications, Video on Demand, sports websites, etc.

    Henrik Rexed is also one of the Organizer of the Conference named Performance Advisory Council , one of the producers of PerfBytes and the owner of the Youtube Channel IsitObservable.
  role: presenter
  title: "The Sound of Code: Instrument with OpenTelemetry"
  abstract: |
    To be efficient in our Continuous testing, continuous deployment approach and on the way we handle production outage, we need to have the right level of observability to understand properly how:

    Our users are interacting with our application
    Our system behaves with significant traffic
    Our infrastructure has been utilize by our application
    …etc
    To help us in our journey, OpenTelemetry has provided a standard on the way we are able to produce and collect measurements in our environments.

    Like any other technology transformation, OpenTelemetry adoption typically starts with small “pet projects” where we usually try to utilize manual or automatic instrumentation. But Opentelemetry is not only about producing traces, it covers the ability to generate metrics and logs from our applications. But like any new emerging framework we tend to wait to get a stable implementation before utilizing it in our production environment.

    We all understand the value of having traces generated out of our application, but the journey could seem difficult and time consuming.

    How can we utilize properly the instruments of Opentelemtry to make our code sound beautiful?

    During this presentation we will explain:
    - The various components of OpenTelemetry
    - The various core Objects required to instrument your code
    - Best practices related to instrumentation
    - The latest news related to OpenTelemetry

  handles:
    twitter: hrexed
    linkedin: hrexed

- id: shai-almog
  name: Shai Almog
  bio: |
    Developer advocate for [Lightrun](https://www.lightrun.com), co-founder of [Codename One](https://www.codenameone.com/), Creator of [DDT](https://github.com/ddtj/ddtj), open source hacker, speaker, author, blogger, Java rockstar and more. ex-Sun/Oracle guy with 30 years of professional development experience. Shai built virtual machines, development tools, mobile phone environments, banking systems, startup/enterprise backends, user interfaces, development frameworks and much more.
  role: presenter
  title: "Debugging at Scale in Production - Deep into your Containers with kubectl debug, KoolKits and Continuous Observability"
  abstract: |
    Brian Kernigham said: “Debugging is twice as hard as writing the code in the first place.”

    In fact, debugging in a modern production environment is even harder - orchestrators spinning containers up and down and weird networking wizardry that keeps everything glued together, make understanding systems that much more difficult than it used to be.

    And, while k8s is well understood by DevOps people by now, it remains a nut that developers are still trying to crack. Where do you start when there’s a production problem? How do you get the tools you’re used to in the remote container? How do you understand what is running where and what is its current state?

    In this talk, we will review debugging a production application deployed to a Kubernetes cluster, and review kubectl debug - a new feature from the Kubernetes sig-cli team. In addition, we’ll review the open source KoolKits project that offers a set of (opinionated) tools for kubectl debug.

    KoolKits builds on top of kubectl debug by adding everything you need right into the image. When logging into a container, we’re often hit with the scarcity of tools at our disposal. No vim (for better or worse), no DB clients, no htop, no debuggers, etc… KoolKits adds all the tools you need right out of the box and lets you inspect a production container easily without resorting to endless installation and configuration cycles for each needed package.

    We’ll finish the talk by delving into how to get better at debugging on a real-world scale. Specifically, we’ll talk about how to be disciplined in our continuous observability efforts by using tools that are built for k8s scale and can run well in those environments, while remaining ergonomic for day to day use.

    This session will go back and forth between explanation slides and demonstration of the topic at hand.
  handles:
    twitter: debugagent
    linkedin: shai-almog-81a42
